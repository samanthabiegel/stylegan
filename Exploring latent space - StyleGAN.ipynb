{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zKw4WhjVAvpp"
   },
   "source": [
    "**Understanding the latent space in Generative Adversarial Networks - Controlled human image generation and editing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "90iIwzraEzvv"
   },
   "source": [
    "# Loading repository and libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "colab_type": "code",
    "id": "JngHzsUzAuA8",
    "outputId": "fd33b569-7f31-42a4-faa7-d4599e4ba7b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'stylegan'...\n",
      "remote: Enumerating objects: 156, done.\u001b[K\n",
      "remote: Total 156 (delta 0), reused 0 (delta 0), pack-reused 156\u001b[K\n",
      "Receiving objects: 100% (156/156), 2.96 MiB | 945.00 KiB/s, done.\n",
      "Resolving deltas: 100% (55/55), done.\n"
     ]
    }
   ],
   "source": [
    "# if running on google colab\n",
    "import os\n",
    "!git clone https://github.com/samanthabiegel/stylegan.git\n",
    "os.chdir(\"stylegan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "colab_type": "code",
    "id": "v945kWhOpbEV",
    "outputId": "22abf0d2-21f0-429c-dce3-c3793e6e2d0e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabling notebook extension jupyter-js-widgets/extension...\r\n",
      "      - Validating: \u001b[32mOK\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "import PIL\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "import os\n",
    "import config\n",
    "import gzip\n",
    "from encoder.generator_model import Generator\n",
    "import dnnlib\n",
    "import dnnlib.tflib as tflib\n",
    "import collections\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Embedding, Flatten, Activation\n",
    "from keras.optimizers import SGD\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interact_manual\n",
    "from IPython.display import display\n",
    "!jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MxepNoS-E_Hp"
   },
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VNOJv40ZErCR"
   },
   "source": [
    "We load a pre-trained StyleGAN generator. This version was trained with Flickr-Faces-HQ dataset at 1024Ã—1024."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "colab_type": "code",
    "id": "rljQtdyQQoRM",
    "outputId": "fc96df02-ef3a-4bf2-c8de-74d4d3fbe6c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://drive.google.com/uc?id=1AenmykaCbvZRMpM2mZj2QScW8eH5mWsW .... done\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Cannot assign a device for operation learnable_dlatents/read: node learnable_dlatents/read (defined at /Users/samanthabiegel/Downloads/stylegan/encoder/generator_model.py:15) was explicitly assigned to /device:GPU:0 but available devices are [ /job:localhost/replica:0/task:0/device:CPU:0 ]. Make sure the device specification refers to a valid device. The requested device appears to be a GPU, but CUDA is not enabled.\n\t [[learnable_dlatents/read]]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1355\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1356\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1357\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1340\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1341\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1428\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1429\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Cannot assign a device for operation learnable_dlatents/read: {{node learnable_dlatents/read}}was explicitly assigned to /device:GPU:0 but available devices are [ /job:localhost/replica:0/task:0/device:CPU:0 ]. Make sure the device specification refers to a valid device. The requested device appears to be a GPU, but CUDA is not enabled.\n\t [[learnable_dlatents/read]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-77493379a322>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# Gs = Long-term average of the generator. Yields higher-quality results than the instantaneous snapshot.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mgenerator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandomize_noise\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Downloads/stylegan/encoder/generator_model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, batch_size, randomize_noise)\u001b[0m\n\u001b[1;32m     25\u001b[0m                                        custom_inputs=[partial(create_variable_for_generator, batch_size=batch_size),\n\u001b[1;32m     26\u001b[0m                                                       partial(create_stub, batch_size=batch_size)],\n\u001b[0;32m---> 27\u001b[0;31m                                        structure='fixed')\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/stylegan/dnnlib/tflib/network.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, input_transform, output_transform, return_as_list, print_progress, minibatch_size, num_gpus, assume_frozen, custom_inputs, *in_arrays, **dynamic_kwargs)\u001b[0m\n\u001b[1;32m    448\u001b[0m             \u001b[0mmb_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmb_end\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmb_begin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m             \u001b[0mmb_in\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmb_begin\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mmb_end\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0msrc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmb_num\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_arrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 450\u001b[0;31m             \u001b[0mmb_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_expr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_expr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmb_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    451\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_arrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmb_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 950\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    951\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1171\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1173\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1174\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1350\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1368\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror_interpolation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1370\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1372\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Cannot assign a device for operation learnable_dlatents/read: node learnable_dlatents/read (defined at /Users/samanthabiegel/Downloads/stylegan/encoder/generator_model.py:15) was explicitly assigned to /device:GPU:0 but available devices are [ /job:localhost/replica:0/task:0/device:CPU:0 ]. Make sure the device specification refers to a valid device. The requested device appears to be a GPU, but CUDA is not enabled.\n\t [[learnable_dlatents/read]]"
     ]
    }
   ],
   "source": [
    "dnnlib.tflib.init_tf()\n",
    "\n",
    "url = 'https://drive.google.com/uc?id=1AenmykaCbvZRMpM2mZj2QScW8eH5mWsW'\n",
    "with dnnlib.util.open_url(url) as f:\n",
    "    _G, _D, Gs = pickle.load(f)\n",
    "    # _G = Instantaneous snapshot of the generator. Mainly useful for resuming a previous training run.\n",
    "    # _D = Instantaneous snapshot of the discriminator. Mainly useful for resuming a previous training run.\n",
    "    # Gs = Long-term average of the generator. Yields higher-quality results than the instantaneous snapshot.\n",
    "    \n",
    "generator = Generator(Gs, batch_size=1, randomize_noise=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jrT-AsobIPoY"
   },
   "source": [
    "Load the training data set. It was created in the following way:\n",
    "\n",
    "* qlatents = np.random.normal(size=(1, 512))\n",
    "* dlatents = Gs_network.components.mapping.run(qlatents, None, minibatch_size=1, randomize_noise=False, structure='fixed')\n",
    "* images = Gs_network.components.synthesis.run(dlatents, minibatch_size=1, randomize_noise=False, output_transform=dict(func=tflib.convert_images_to_uint8, nchw_to_nhwc=True), structure='fixed')\n",
    "\n",
    "\n",
    "So qlatent_data represents the random input noise and dlatent_data represents the images in the intermediate latent space W. Labels_data represents the training labels, and includes the facial attribute values for each image, as well as facial landmark coordinates and the facial bounding box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "colab_type": "code",
    "id": "K9pMsK3jBBi-",
    "outputId": "c1668e0d-648b-47eb-a1ab-c3acc00af3e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://drive.google.com/uc?id=1xMM3AFq0r014IIhBLiMCjKJJvbhLUQ9t .... done\n"
     ]
    }
   ],
   "source": [
    "LATENT_TRAINING_DATA = 'https://drive.google.com/uc?id=1xMM3AFq0r014IIhBLiMCjKJJvbhLUQ9t'\n",
    "    \n",
    "with dnnlib.util.open_url(LATENT_TRAINING_DATA, cache_dir=config.cache_dir) as f:\n",
    "    qlatent_data, dlatent_data, labels_data = pickle.load(gzip.GzipFile(fileobj=f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F8F2L0pMrFrH"
   },
   "source": [
    "Download dictionary of directions I've trained previously. Later on code is provided to obtain these directions, so they can easily modified. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WyGuOsEUrEyV"
   },
   "outputs": [],
   "source": [
    "with open('directions_pretrained.p', 'rb') as handle:\n",
    "    directions = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lvv3TeGYRO_w"
   },
   "source": [
    "# Data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "phsfGQ-HI2Cz"
   },
   "source": [
    "Process the training labels data to get a facial attributes dictionary for each image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "colab_type": "code",
    "id": "zBalzF9eAnBg",
    "outputId": "f446dcef-71da-46e6-ec5f-0eb3a94127f8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accessories': [],\n",
       " 'age': 50.0,\n",
       " 'blur_blurLevel': 'low',\n",
       " 'blur_value': 0.06,\n",
       " 'emotion_anger': 0.0,\n",
       " 'emotion_contempt': 0.0,\n",
       " 'emotion_disgust': 0.0,\n",
       " 'emotion_fear': 0.0,\n",
       " 'emotion_happiness': 0.999,\n",
       " 'emotion_neutral': 0.001,\n",
       " 'emotion_sadness': 0.0,\n",
       " 'emotion_surprise': 0.0,\n",
       " 'exposure_exposureLevel': 'goodExposure',\n",
       " 'exposure_value': 0.71,\n",
       " 'facialHair_beard': 0.1,\n",
       " 'facialHair_moustache': 0.1,\n",
       " 'facialHair_sideburns': 0.1,\n",
       " 'gender': 'male',\n",
       " 'glasses': 'NoGlasses',\n",
       " 'hair_bald': 0.11,\n",
       " 'hair_hairColor_black': 0.23,\n",
       " 'hair_hairColor_blond': 0.36,\n",
       " 'hair_hairColor_brown': 1.0,\n",
       " 'hair_hairColor_gray': 0.65,\n",
       " 'hair_hairColor_other': 0.04,\n",
       " 'hair_hairColor_red': 0.2,\n",
       " 'hair_invisible': False,\n",
       " 'headPose_pitch': 0.0,\n",
       " 'headPose_roll': -0.4,\n",
       " 'headPose_yaw': 3.1,\n",
       " 'makeup_eyeMakeup': False,\n",
       " 'makeup_lipMakeup': False,\n",
       " 'noise_noiseLevel': 'low',\n",
       " 'noise_value': 0.09,\n",
       " 'occlusion_eyeOccluded': False,\n",
       " 'occlusion_foreheadOccluded': False,\n",
       " 'occlusion_mouthOccluded': False,\n",
       " 'smile': 0.999}"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def flatten(d, parent_key='', sep='_'):\n",
    "    items = []\n",
    "    for k, v in d.items():\n",
    "        new_key = parent_key + sep + k if parent_key else k\n",
    "        if isinstance(v, collections.MutableMapping):\n",
    "            items.extend(flatten(v, new_key, sep=sep).items())\n",
    "        else:\n",
    "            items.append((new_key, v))\n",
    "    return dict(items)\n",
    "  \n",
    "attribute_dict = labels_data.copy()\n",
    "  \n",
    "for i, image in enumerate(attribute_dict):\n",
    "  attribute_dict[i] = flatten(attribute_dict[i]['faceAttributes'])\n",
    "  for item in attribute_dict[i]['hair_hairColor']:\n",
    "    color = item['color']\n",
    "    value = item['confidence']\n",
    "    attribute_dict[i]['hair_hairColor_' + color] = value\n",
    "  attribute_dict[i].pop('hair_hairColor')\n",
    "\n",
    "# Optionally, show the training targets for the first image\n",
    "attribute_dict[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YdQ2W3kkKT4d"
   },
   "source": [
    "Create a few dictionaries for analyzing the training data. \n",
    "\n",
    "* y_data is a dictionary with an array of values for each attribute key.\n",
    "* count_classes counts the number of unique values for each attribute key. This can be used to distinguish between continuous and discrete variables.\n",
    "* type_dict lists the variable type for each attribute key.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2vWn133kJAgV"
   },
   "outputs": [],
   "source": [
    "y_data = {}\n",
    "attribute_list = []\n",
    "\n",
    "for feature in attribute_dict[0]:\n",
    "  attribute_list.append(feature)\n",
    "  var_name = 'y_' + feature\n",
    "  y_data[var_name] = np.array([x[feature] for x in attribute_dict if feature in x])\n",
    "  \n",
    "count_classes = {}\n",
    "\n",
    "y_data.pop('y_accessories')\n",
    "\n",
    "for feature in y_data:\n",
    "  count = len(np.unique(y_data[feature]))\n",
    "  count_classes[feature] = count \n",
    "  \n",
    "type_dict = {}\n",
    "for feature in attribute_dict[0]:\n",
    "  type_dict[feature] = type(attribute_dict[0][feature])\n",
    "  \n",
    "# Optionally, print the dictionaries created\n",
    "# print(y_data)\n",
    "# print(count_classes)\n",
    "# print(type_dict)\n",
    "# print(attribute_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "colab_type": "code",
    "id": "DbFrNUoN3Bs9",
    "outputId": "1ebc6f68-7fa4-4d8c-d9c0-672ed298fc3a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0. , 0.1, 0.4, 0.6, 0.9])"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_data['y_facialHair_beard'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P0OfjeRqoQXA"
   },
   "source": [
    "Create the input data set X_data and a different data set for the hair variables, since not all training images have information on the hair. Then create the target variables:\n",
    "* if the target variable is continuous, calculate the mean for that variable accross all training images that have information on that variable. Then set the target to 1 for each image that is above the mean, and to 0 for each image that is below the mean.\n",
    "* if the target variable is boolean, it's already binary and we can just train on the value.\n",
    "* for the glasses attribute, we set it to 1 if the value is not 'NoGlasses' and to 0 otherwise.\n",
    "* for the gender attribute, we set it to 1 if the value is 'male' and to 0 otherwise.\n",
    "* 'headpose_pitch' has only one value, so we ignore it.\n",
    "* ''blur_value', 'exposure_value', and 'noise_value' are not necessarily features we want to change, so we ignore it for now as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XbTSdrgKC8tT"
   },
   "outputs": [],
   "source": [
    "X_data = dlatent_data.reshape((-1, 18*512))\n",
    "\n",
    "has_blondhair = ['hair_hairColor_blond' in x for x in attribute_dict]\n",
    "X_hair_data = X_data[has_blondhair,:]\n",
    "\n",
    "y_binary = {}\n",
    "\n",
    "for attribute in attribute_list:\n",
    "  \n",
    "  if type_dict[attribute] == float:\n",
    "    has_attribute = [attribute in x for x in attribute_dict]\n",
    "    X_withattribute_data = X_data[has_attribute,:]\n",
    "    if X_withattribute_data.shape[0] == X_data.shape[0]:\n",
    "      mean_attribute = sum(y_data['y_'+attribute])/X_data.shape[0]\n",
    "      y_binary[attribute] = np.array([x[attribute] > mean_attribute for x in attribute_dict])\n",
    "    if X_withattribute_data.shape[0] != X_data.shape[0]:\n",
    "      mean_attribute = sum(y_data['y_'+attribute])/X_hair_data.shape[0]\n",
    "      y_binary[attribute] = np.array([x[attribute] > mean_attribute for x in attribute_dict if attribute in x])\n",
    "      \n",
    "  if type_dict[attribute] == bool:\n",
    "    y_binary[attribute] = np.array([x[attribute] for x in attribute_dict])\n",
    "    \n",
    "  if attribute == 'glasses':\n",
    "    y_binary['glasses'] = np.array([x['glasses'] != 'NoGlasses' for x in attribute_dict])\n",
    "\n",
    "  if attribute == 'gender':\n",
    "    y_binary['gender'] = np.array([x['gender'] == 'male' for x in attribute_dict])\n",
    "    \n",
    "for key in ['headPose_pitch', 'blur_value', 'exposure_value', 'noise_value']:\n",
    "  y_binary.pop(key)\n",
    "    \n",
    "# Optionally, print the new dictionary of targets\n",
    "# print(y_binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bi_9tPGGr3Q3"
   },
   "source": [
    "# Learning directions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UPQq8kBFt-cv"
   },
   "source": [
    "Now that we have binary targets for all of the variables, we can loop through all of them, and train a logistic regression model per attribute. \n",
    "\n",
    "If you want to save the new directions for later use, use the bottom part to download it as a file. Then push to the repository as 'directions_pretrained'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D7eAPsL_HnG5"
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# directions = {}\n",
    "\n",
    "# for feature in y_binary.keys():\n",
    "#   print(\"Training:\", feature)\n",
    "#   if feature not in directions:\n",
    "#     clf = LogisticRegression(class_weight='balanced', solver='lbfgs')\n",
    "#     if len(y_binary[feature]) != X_data.shape[0]:\n",
    "#       clf.fit(X_hair_data.reshape((-1, 18*512)), y_binary[feature])\n",
    "#     else:\n",
    "#       clf.fit(X_data.reshape((-1, 18*512)), y_binary[feature])\n",
    "#     directions[feature] = clf.coef_.reshape((18, 512))\n",
    "\n",
    "# use below to save\n",
    "# with open('directions.p', 'wb') as handle:\n",
    "#     pickle.dump(directions, handle)\n",
    "\n",
    "# from google.colab import files\n",
    "# files.download('directions.p') \n",
    "\n",
    "# Optionally, print dictionary of learned directions\n",
    "# print(directions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HMa5Mn-7GtFx"
   },
   "source": [
    "# Finding latent representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RgiMcl4o-YSx"
   },
   "source": [
    "Here we process any images that are in the folder raw_images. If you want to experiment with your own images, just add them to this folder. We first extract and align faces from the images. Then we find the latent representations of these aligned images.\n",
    "\n",
    "If you want to save the latent vectors for later use, download and save them to the latent_representations folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IgN8b5JCct0Z"
   },
   "outputs": [],
   "source": [
    "# os.mkdir('aligned_images')\n",
    "# !python align_images.py raw_images/ aligned_images/\n",
    "# !python encode_images.py aligned_images/ generated_images/ latent_representations/\n",
    "\n",
    "# replace name for own images\n",
    "# justinbieber = np.load('latent_representations/justinbieber_01.npy')\n",
    "# os.chdir('latent_representations')\n",
    "# from google.colab import files\n",
    "# files.download('justinbieber_01.npy')\n",
    "# %cd ..\n",
    "\n",
    "justinbieber = np.load('ffhq_dataset/latent_representations/justinbieber_01.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h_fxRCGiBN9W"
   },
   "source": [
    "We also create a latent vector of all zeros. This represents the 'average face' of the dataset. Since we sample from a random normal distribution, the expected vector z is all zeros, which are not affected by the mapping network, so in intermediate latent space W it's all zeros as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Svqr6crnBPD_"
   },
   "outputs": [],
   "source": [
    "zero_latent_vector = np.zeros((18,512))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Jvb5wiycv5It"
   },
   "source": [
    "# Preparing to visualize results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5j_5qflP6HNA"
   },
   "source": [
    "Create functions that visualize the results of the shifts. Shift_latent loops through the list of features to change, and for each feature shifts the latent vector into that direction with a magnitude specified by the user. The final image is then shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e-NsT2qOvI-X"
   },
   "outputs": [],
   "source": [
    "def generate_image(latent_vector):\n",
    "    latent_vector = latent_vector.reshape((1, 18, 512))\n",
    "    generator.set_dlatents(latent_vector)\n",
    "    img_array = generator.generate_images()[0]\n",
    "    img = PIL.Image.fromarray(img_array, 'RGB')\n",
    "    return img.resize((256, 256))\n",
    "\n",
    "def shift_latent(latent_vector, key_dict, coeff_dict, feature_list, directions, n_coeffs):\n",
    "  \n",
    "    fig,ax = plt.subplots(1, n_coeffs, figsize=(15, 10), dpi=80)\n",
    "    \n",
    "    for i in range(n_coeffs):\n",
    "      \n",
    "        new_latent_vector = latent_vector.copy()\n",
    "        for key in feature_list:\n",
    "          coeff = coeff_dict[key][i]\n",
    "          if key_dict[key] == 'coarse':\n",
    "            new_latent_vector[:4] = (new_latent_vector + coeff*directions[key])[:4]\n",
    "          if key_dict[key] == 'middle':\n",
    "            new_latent_vector[4:8] = (new_latent_vector + coeff*directions[key])[4:8]\n",
    "          if key_dict[key] == 'coarse_middle':\n",
    "            new_latent_vector[:8] = (new_latent_vector + coeff*directions[key])[:8]\n",
    "          if key_dict[key] == 'fine':\n",
    "            new_latent_vector[8:] = (new_latent_vector + coeff*directions[key])[8:]\n",
    "        \n",
    "        ax[i].imshow(generate_image(new_latent_vector))\n",
    "    [x.axis('off') for x in ax]\n",
    "    \n",
    "    if n_coeffs == 2:\n",
    "      ax[0].set_title('original')\n",
    "      ax[1].set_title('shifted')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HScjAndS6niO"
   },
   "source": [
    "Create reference dictionaries:\n",
    "* key_dict maps the attributes to the resolutions their shifts should be applied to. The attributes were assigned to a resolution level based on information in the paper as well as experimenting. If the shifts for an attribute don't work well, try assigning the attribute to a different resolution level.\n",
    "* change_dict maps the attributes to a list that specifies by how much we want to shift the latent vector into the direction of the attribute. We start with all zeros, because we don't want to change anything unless specified later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZhoDmpxMqP_S"
   },
   "outputs": [],
   "source": [
    "key_dict = {}\n",
    "\n",
    "for key in ['glasses', 'headPose_roll', 'headPose_yaw', 'occlusion_foreheadOccluded', 'occlusion_mouthOccluded', 'occlusion_eyeOccluded']:\n",
    "  key_dict[key] = 'coarse'\n",
    "for key in ['makeup_eyeMakeup', 'makeup_lipMakeup']:\n",
    "  key_dict[key] = 'middle'\n",
    "for key in ['age', 'smile', 'gender', 'facialHair_beard', 'facialHair_moustache', 'facialHair_sideburns', 'emotion_anger', 'emotion_contempt', 'emotion_disgust', \n",
    "            'emotion_fear', 'emotion_happiness', 'emotion_neutral', 'emotion_sadness', 'emotion_surprise', 'hair_bald']:\n",
    "  key_dict[key] = 'coarse_middle'\n",
    "for key in ['hair_hairColor_black', 'hair_hairColor_blond', 'hair_hairColor_brown', 'hair_hairColor_red', 'hair_hairColor_gray', 'hair_hairColor_other']:\n",
    "  key_dict[key] = 'fine'\n",
    "  \n",
    "change_dict = {}\n",
    "for key in y_binary.keys():\n",
    "  change_dict[key] = [0,0]\n",
    "  \n",
    "# Optionally, show dictionaries\n",
    "# print(key_dict)\n",
    "# print(change_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MRJzcsP_HAsH"
   },
   "source": [
    "# Interactive visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4MNpBrhJEEik"
   },
   "source": [
    "Create interactive interface to experiment with shifting into different directions. It visualizes to images: the original on the left and the image with all changes applied on the right.\n",
    "\n",
    "The range of possible magnitudes is set for each attribute after interact_manual, and can be altered by changing the min or max value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "colab_type": "code",
    "id": "r-zoGUj_IEKZ",
    "outputId": "8dd0a260-4976-411f-8b61-4130444d2f68"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0abff3923fef412b819495fdf63d25fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.0, description='smile', max=1.0, min=-1.0, readout=False), FloatSlidâ€¦"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact_manual(smile=widgets.FloatSlider(min=-1.0, max=1.0, readout=False), \n",
    "                 age=widgets.FloatSlider(min= -3, max=3, readout=False), \n",
    "                 emotion=['happiness', 'anger', 'contempt', 'disgust', 'fear', 'neutral', 'surprise', 'sadness'],\n",
    "                 m_emotion=widgets.FloatSlider(min = 0, max=2, readout=False, description='emotion intensity'),\n",
    "                 gender=widgets.FloatSlider(min=-2.0, max=2.0, description='female-male', readout=False),\n",
    "                 beard=widgets.FloatSlider(min=-2.0, max=2.0, readout=False),\n",
    "                 moustache=widgets.FloatSlider(min=-2.0, max=2.0, readout=False),\n",
    "                 glasses=['remove', '', 'add'],\n",
    "                 m_glasses=widgets.FloatSlider(min=0.0, max=1.0, description='glasses intensity', readout=False),\n",
    "                 haircolor=['black', 'blond', 'brown', 'red', 'gray', 'other', ''],\n",
    "                 bald=widgets.FloatSlider(min=-2.0, max=2.0, description='baldness', readout=False),\n",
    "                 yaw=widgets.FloatSlider(min=-1.0, max=1.0, description='headpose yaw', readout=False),\n",
    "                 roll=widgets.FloatSlider(min=-1.0, max=1.0, description='headpose roll', readout=False),\n",
    "                 eyemakeup=widgets.FloatSlider(min=-2.0, max=2.0, description='eye makeup', readout=False),\n",
    "                 lipmakeup=widgets.FloatSlider(min=-2.0, max=2.0, description='lip makeup', readout=False))\n",
    "          \n",
    "\n",
    "def play_with_shifts_light(smile=0, age=0, gender=0, beard=0, moustache=0, emotion='neutral', m_emotion=0, glasses='', m_glasses=0, haircolor='', bald=0, \n",
    "                      yaw=0, roll=0, eyemakeup=0, lipmakeup=0):\n",
    "  n_coeffs = 2\n",
    "  \n",
    "  current_change_dict = change_dict.copy()\n",
    "   \n",
    "  current_change_dict['age'][1] = age\n",
    "  current_change_dict['smile'][1] = smile\n",
    "  current_change_dict['facialHair_beard'][1] = beard\n",
    "  current_change_dict['facialHair_moustache'][1] = moustache\n",
    "  current_change_dict['emotion_'+emotion][1] = m_emotion\n",
    "  current_change_dict['gender'][1] = gender\n",
    "  current_change_dict['hair_bald'][1] = bald\n",
    "  current_change_dict['headPose_yaw'][1] = yaw\n",
    "  current_change_dict['headPose_roll'][1] = roll\n",
    "  current_change_dict['makeup_eyeMakeup'][1] = eyemakeup\n",
    "  current_change_dict['makeup_lipMakeup'][1] = lipmakeup\n",
    "\n",
    "  if glasses == 'add':\n",
    "    current_change_dict['glasses'][1] = m_glasses\n",
    "  if glasses == 'remove':\n",
    "    current_change_dict['glasses'][1] = -m_glasses\n",
    "  if haircolor != '':\n",
    "    current_change_dict['hair_hairColor_'+haircolor][1] = 1\n",
    "  \n",
    "  change_features = []\n",
    "  for key in current_change_dict:\n",
    "    if np.count_nonzero(current_change_dict[key]) != 0:\n",
    "      change_features.append(key)\n",
    "        \n",
    "  shift_latent(after_mapping, key_dict, current_change_dict, change_features, directions, n_coeffs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5PNbAcLkJTos"
   },
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Btwi-I2LJVON"
   },
   "source": [
    "A different approach to creating the targets for hair color. Instead of setting 1 if the value is above the mean for that hair color, we compare the value to the other hair colors for that image. Only the hair color attribute with the highest value gets a 1, the others get a zero. Add this to the attribute_dict creation code to implement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oPXWPEgkJS-l"
   },
   "outputs": [],
   "source": [
    "#   haircolor_confidence = [ v for k,v in attribute_dict[i].items() if 'hair_hairColor' in k]\n",
    "#   haircolors = [ k for k,v in attribute_dict[i].items() if 'hair_hairColor' in k]\n",
    "#   if haircolors == []:\n",
    "#     continue\n",
    "#   max_haircolor = haircolors[np.argmax(np.array(haircolor_confidence))]\n",
    "#   for color in haircolors:\n",
    "#     attribute_dict[i][color] = False\n",
    "#   attribute_dict[i][max_haircolor] = True\n",
    "  \n",
    "# attribute_dict.pop('hair_hairColor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jT5FF161mlV8"
   },
   "source": [
    "Create different data sets for different types of glasses. The goal was to train separate models for normal glasses and sunglasses. However, the sunglass model did not work well at all, probably due to the small training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QBrLN74Umljz"
   },
   "outputs": [],
   "source": [
    "# has_readingglasses = np.array([x['glasses'] == 'ReadingGlasses' for x in attribute_dict])\n",
    "# has_noglasses = np.array([x['glasses'] == 'NoGlasses' for x in attribute_dict])\n",
    "# has_sunglasses = np.array([x['glasses'] == 'Sunglasses' for x in attribute_dict])\n",
    "# glasses_data = has_readingglasses | has_noglasses\n",
    "# X_glasses_data = X_data[glasses_data,:]\n",
    "# sunglasses_data = has_sunglasses | has_noglasses\n",
    "# X_sunglasses_data = X_data[sunglasses_data,:]\n",
    "\n",
    "# add this to the attribute_list loop\n",
    "# if attribute == 'glasses':\n",
    "#   y_binary['glasses'] = np.array([x['glasses'] == 'ReadingGlasses' for x in attribute_dict if x['glasses'] == 'ReadingGlasses' or x['glasses'] == 'NoGlasses'])\n",
    "#   y_binary['sunglasses'] = np.array([x['glasses'] == 'Sunglasses' for x in attribute_dict if x['glasses'] == 'NoGlasses' or x['glasses'] == 'Sunglasses'])\n",
    "\n",
    "# use this for training the targets\n",
    "# for feature in y_binary.keys():\n",
    "#   clf = LogisticRegression(class_weight='balanced', solver='lbfgs')\n",
    "#   if feature == 'glasses':\n",
    "#     clf.fit(X_glasses_data.reshape((-1, 18*512)), y_binary[feature])\n",
    "#   elif feature == 'sunglasses':\n",
    "#     clf.fit(X_sunglasses_data.reshape((-1, 18*512)), y_binary[feature])\n",
    "\n",
    "#   directions[feature] = clf.coef_.reshape((18, 512))    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u-jo7uP6m6_H"
   },
   "source": [
    "Create different targets for the beard model. Since there are only 5 classes for the beard variable at different levels, I tried training a model for each of these classes. I could then use the coefficients of any of these models to obtain the beard direction. Nevertheless, I found that it worked better to just create targets based on the mean beard value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xCXplc9Wm7I1"
   },
   "outputs": [],
   "source": [
    "# y_beard_data = np.array([x['facialHair_beard'] for x in attribute_dict])\n",
    "# y_beard_data = y_beard_data*10\n",
    "# y_beard_data.astype(int)\n",
    "# y_blackhair_data = np.array([x['hair_hairColor_black'] for x in attribute_dict if 'hair_hairColor_black' in x])\n",
    "# y_blondhair_data = np.array([x['hair_hairColor_blond'] for x in attribute_dict if 'hair_hairColor_blond' in x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2Wu9Be4VsBlk"
   },
   "source": [
    "We could also use the pretrained directions for some of the attributes instead of using our own. I found it makes no difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k133XixdsAsW"
   },
   "outputs": [],
   "source": [
    "# directions['smile'] = np.load('ffhq_dataset/latent_directions/smile.npy')\n",
    "# directions['gender'] = np.load('ffhq_dataset/latent_directions/gender.npy')\n",
    "# directions['age'] = np.load('ffhq_dataset/latent_directions/age.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_KbauCKTtlcx"
   },
   "source": [
    "Experiments with other models for finding directions. They all took very long to train and did not give good results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EdxweIh5tkq0"
   },
   "outputs": [],
   "source": [
    "# clf = SGDClassifier('log', class_weight='balanced')\n",
    "# clf = LogisticRegression(solver='lbfgs', multi_class='multinomial')\n",
    "# clf.fit(X_data.reshape((-1, 18*512)), y_beard_data)\n",
    "# directions['beard'] = clf.coef_.reshape((-1, 18, 512))\n",
    "\n",
    "# clf = LinearRegression()\n",
    "# clf.fit(X_data.reshape((-1, 18*512)), y_bald_data)\n",
    "# directions['baldness'] = clf.coef_.reshape((18, 512))\n",
    "\n",
    "# clf = SVC()\n",
    "# clf.fit(X_data.reshape((-1, 18*512)), y_eyemakeup_data)\n",
    "# directions['eyemakeup'] = clf.coef_.reshape((18, 512))\n",
    "\n",
    "# rgr = RandomForestRegressor()\n",
    "# rgr.fit(X_data[:1000,:].reshape((-1, 18*512)), y_bald_data[:1000])\n",
    "# bald_direction = rgr.coef_.reshape((18,512))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4n0JET2tsJwG"
   },
   "source": [
    "A different method of visualization. Here only one attribute at a time can be changed. Change the row numbers for the new latent vector to change the resolutions the shift should be applied to (0:4 for coarse, 4:8 for middle)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UclvX1FK0eia"
   },
   "outputs": [],
   "source": [
    "# def move_and_show(latent_vector, direction, coeffs):\n",
    "#     fig,ax = plt.subplots(1, len(coeffs), figsize=(15, 10), dpi=80)\n",
    "#     for i, coeff in enumerate(coeffs):\n",
    "#         new_latent_vector = latent_vector.copy()\n",
    "#         new_latent_vector[:8] = (latent_vector + coeff*direction)[:8]\n",
    "#         ax[i].imshow(generate_image(new_latent_vector))\n",
    "#         ax[i].set_title('Coeff: %0.1f' % coeff)\n",
    "#     [x.axis('off') for x in ax]\n",
    "#     plt.show()\n",
    "\n",
    "# for i in range(2):\n",
    "#     move_and_show(X_data.reshape((-1, 18, 512))[i], directions['gender'], [-4, -2, 0, 2, 4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qtdumBTLwbud"
   },
   "source": [
    "The next code should be part of the shift_latent function. It explores the idea of shifting for all emotion directions when the emotion is changed. So if we want to increasse for one emotion, we also decrease for the others. This only lead to weirder results though.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7SSyhXwWwb7G"
   },
   "outputs": [],
   "source": [
    "#     extended_feature_list = feature_list.copy()\n",
    "#     for key in feature_list:\n",
    "#       if 'emotion' in key:\n",
    "#         emotion_list = ['emotion_anger', 'emotion_contempt', 'emotion_disgust', \n",
    "#             'emotion_fear', 'emotion_happiness', 'emotion_neutral', 'emotion_sadness', 'emotion_surprise']\n",
    "#         for emotion in emotion_list:\n",
    "#           if emotion != key:\n",
    "#             coeff_dict[emotion] = [-1 * i for i in coeff_dict[key]]\n",
    "#         emotion_list.remove(key)\n",
    "#         extended_feature_list.extend(emotion_list)\n",
    "\n",
    "#     feature_list = extended_feature_list.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PUhleT4l5uuq"
   },
   "source": [
    "Handcrafted dictionary of optimal values for change coefficients for each attribute. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "opOYpC205u4w"
   },
   "outputs": [],
   "source": [
    "# old_change_dict = {'makeup_eyeMakeup': [-6, 0, 6], \n",
    "#                'makeup_lipMakeup': [-2, 0, 2],\n",
    "#                'age': [-2, 0, 2],\n",
    "#                'gender': [-2, 0, 2],\n",
    "#                'smile': [-2, 0, 2],\n",
    "#                'headPose_roll': [0, 2, 4],\n",
    "#                'headPose_yaw': [-1.5, 0, 1.5],\n",
    "#                'facialHair_moustache': [-2, 0, 2],\n",
    "#                'facialHair_beard': [-2, 0, 2],\n",
    "#                'facialHair_sideburns': [-2, 0, 2],\n",
    "#                'glasses': [-1, 0, 1],\n",
    "#                'sunglasses': [-2, 0, 2],\n",
    "#                'emotion_anger': [0, 2, 4],\n",
    "#                'emotion_contempt': [0, 2, 4],\n",
    "#                'emotion_disgust': [0, 2, 4],\n",
    "#                'emotion_fear': [0, 0.5, 1],\n",
    "#                'emotion_happiness': [0, 2, 4],\n",
    "#                'emotion_neutral': [0, 2, 4],\n",
    "#                'emotion_sadness': [0, 2, 4],\n",
    "#                'emotion_surprise': [0, 2, 4],\n",
    "#                'occlusion_foreheadOccluded': [-2, 0, 2],\n",
    "#                'occlusion_eyeOccluded': [-2, 0, 2],\n",
    "#                'occlusion_mouthOccluded': [-2, 0, 2],\n",
    "#                'hair_bald': [-2, 0, 2],\n",
    "#                'hair_invisible': [-2, 0, 2],\n",
    "#                'hair_hairColor_brown': [-2, 0, 2],\n",
    "#                'hair_hairColor_blond': [-2, 0, 2],\n",
    "#                'hair_hairColor_black': [-2, 0, 2],\n",
    "#                'hair_hairColor_red': [-2, 0, 2],\n",
    "#                'hair_hairColor_gray': [-2, 0, 2],\n",
    "#                'hair_hairColor_other': [-2, 0, 2]\n",
    "#               }\n",
    "\n",
    "# feature_list = ['gender']\n",
    "# n_coeffs = 3\n",
    "\n",
    "# for i in range(20):\n",
    "#   shift_latent(X_data.reshape((-1, 18, 512))[i], key_dict, old_change_dict, feature_list, directions, n_coeffs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gF_HjlUe9XRf"
   },
   "source": [
    "This is a small start of implementing the text2image part of the model. It takes a text string as user input, and saves it as a variable. From here it is pretty straightforward to analyze the input text, and determine which features should be changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fWBsf7Cr6Uv-"
   },
   "outputs": [],
   "source": [
    "# input_text = widgets.Text()\n",
    "# output_text = widgets.Text()\n",
    "\n",
    "# def handle_submit(sender):\n",
    "#   output_text.value = input_text.value\n",
    "#   print(output_text.value)\n",
    "  \n",
    "# input_text.on_submit(handle_submit)\n",
    "\n",
    "# input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mfvGIr8V9Mdb"
   },
   "outputs": [],
   "source": [
    "# description = output_text.value\n",
    "# description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qYj1VkCx_48p"
   },
   "source": [
    "Explore latent directions and representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N8kgPQozwt5K"
   },
   "outputs": [],
   "source": [
    "# import seaborn\n",
    "# import scipy\n",
    "\n",
    "# plt.figure(figsize=(20,10))\n",
    "# # heatmap = seaborn.heatmap(X_data[0,:].reshape(18,512)+directions['glasses'])\n",
    "# heatmap = seaborn.heatmap(justinbieber)\n",
    "# plt.show()\n",
    "\n",
    "# plt.figure(figsize=(20,10))\n",
    "# heatmap = seaborn.heatmap(X_data[0,:].reshape(18,512))\n",
    "# # heatmap = seaborn.heatmap(taylor_swift)\n",
    "# plt.show()\n",
    "\n",
    "# plt.figure(figsize=(20,10))\n",
    "# heatmap = seaborn.heatmap(X_data[0,:].reshape(18,512)-4*directions['gender'])\n",
    "# # heatmap = seaborn.heatmap(taylor_swift)\n",
    "# plt.show()\n",
    "\n",
    "# plt.figure(figsize=(20,10))\n",
    "# heatmap = seaborn.heatmap(directions['gender'])\n",
    "# # heatmap = seaborn.heatmap(taylor_swift)\n",
    "# plt.show()\n",
    "\n",
    "# zero_latent_vector = scipy.sparse.random(m=18,n=512, density = .1)\n",
    "# # zero_latent_vector = np.array(zero_latent_vector)\n",
    "\n",
    "# new_latent_vector = np.random.normal(size=(1,512))\n",
    "# after_mapping = Gs.components.mapping.run(new_latent_vector, None, minibatch_size=1, randomize_noise=False, structure='fixed')\n",
    "# new_latent_vector = np.ones((18,512))-0.9\n",
    "# print(new_latent_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7y4ElOolDU8Q"
   },
   "source": [
    "Interface with all attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jm-517oU-45K"
   },
   "outputs": [],
   "source": [
    "# @interact_manual(smile=widgets.FloatSlider(min=-1.0, max=1.0, readout=False), \n",
    "#                  age=widgets.FloatSlider(min= -3, max=3, readout=False), \n",
    "#                  emotion=['happy', 'anger', 'contempt', 'disgust', 'fear', 'neutral', 'surprise', 'sadness'],\n",
    "#                  m_emotion=widgets.FloatSlider(min = 0, max=2, readout=False, description='emotion intensity'),\n",
    "#                  gender=widgets.FloatSlider(min=-2.0, max=2.0, description='female-male', readout=False),\n",
    "#                  beard=widgets.FloatSlider(min=-2.0, max=2.0, readout=False),\n",
    "#                  moustache=widgets.FloatSlider(min=-2.0, max=2.0, readout=False),\n",
    "#                  glasses=['remove', '', 'add'],\n",
    "#                  m_glasses=widgets.FloatSlider(min=0.0, max=1.0, description='glasses intensity', readout=False),\n",
    "#                  haircolor=['black', 'blond', 'brown', 'red', 'gray', 'other', ''],\n",
    "#                  bald=widgets.FloatSlider(min=-1.0, max=1.0, description='baldness', readout=False),\n",
    "#                  yaw=widgets.FloatSlider(min=-1.0, max=1.0, description='headpose yaw', readout=False),\n",
    "#                  eyemakeup=widgets.FloatSlider(min=-1.0, max=1.0, description='eye makeup', readout=False),\n",
    "#                  lipmakeup=widgets.FloatSlider(min=-1.0, max=1.0, description='lip makeup', readout=False))\n",
    "          \n",
    "\n",
    "\n",
    "# def play_with_shifts(smile=0, age=0, gender=0, beard=0, moustache=0, sideburns=0, emotion='neutral', m_emotion=0, glasses='', m_glasses=0, haircolor='', bald=0, \n",
    "#                      hairvisible=0, yaw=0, roll=0, eyemakeup=0, lipmakeup=0, foreheadocc=0, eyeocc=0, mouthocc=0):\n",
    "#   n_coeffs = 2\n",
    "  \n",
    "#   current_change_dict = change_dict.copy()\n",
    "   \n",
    "#   current_change_dict['age'][1] = age\n",
    "#   current_change_dict['smile'][1] = smile\n",
    "#   current_change_dict['facialHair_beard'][1] = beard\n",
    "#   current_change_dict['facialHair_moustache'][1] = moustache\n",
    "#   current_change_dict['facialHair_sideburns'][1] = sideburns\n",
    "#   current_change_dict['emotion_'+emotion][1] = m_emotion\n",
    "#   current_change_dict['gender'][1] = gender\n",
    "#   current_change_dict['hair_bald'][1] = bald\n",
    "#   current_change_dict['hair_invisible'][1] = hairvisible\n",
    "#   current_change_dict['headPose_yaw'][1] = yaw\n",
    "#   current_change_dict['headPose_roll'][1] = roll\n",
    "#   current_change_dict['makeup_eyeMakeup'][1] = eyemakeup\n",
    "#   current_change_dict['makeup_lipMakeup'][1] = lipmakeup\n",
    "#   current_change_dict['occlusion_foreheadOccluded'][1] = foreheadocc\n",
    "#   current_change_dict['occlusion_eyeOccluded'][1] = eyeocc\n",
    "#   current_change_dict['occlusion_mouthOccluded'][1] = mouthocc\n",
    "\n",
    "#   if glasses == 'add':\n",
    "#     current_change_dict['glasses'][1] = m_glasses\n",
    "#   if glasses == 'remove':\n",
    "#     current_change_dict['glasses'][1] = -m_glasses\n",
    "#   if haircolor != '':\n",
    "#     current_change_dict['hair_hairColor_'+haircolor][1] = 1\n",
    "  \n",
    "#   change_features = []\n",
    "#   for key in current_change_dict:\n",
    "#     if np.count_nonzero(current_change_dict[key]) != 0:\n",
    "#       change_features.append(key)\n",
    "        \n",
    "#   shift_latent(zero_latent_vector, key_dict, current_change_dict, change_features, directions, n_coeffs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XN-elwVlA2kE"
   },
   "source": [
    "Test a nonlinear model. It worked well too, but is much less practical to use than a linear model, where we just get coefficients to shift the latent vector by."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pGK9SmLlr9Lz"
   },
   "outputs": [],
   "source": [
    "# model = Sequential()\n",
    "# model.add(Dense(32, activation='relu'))\n",
    "# model.add(Dropout(0.3))\n",
    "# model.add(Dense(1))\n",
    "# # model.add(Activation('sigmoid'))\n",
    "# model.compile('adam', 'mean_squared_error', metrics=['accuracy'])\n",
    "\n",
    "# model.fit(X_data.reshape((-1, 18*512)), y_beard_data, validation_split=0.2, epochs=5)\n",
    "# model = Model(model.input, model.layers[-1].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "az9iNmwuauq-"
   },
   "outputs": [],
   "source": [
    "# embedding_model = Sequential()\n",
    "# embedding_model.add(Embedding(10, 18*512, input_length=1)) # it's actually just a variable\n",
    "# embedding_model.add(Flatten())\n",
    "\n",
    "# nonlinear_beard_model = Model(embedding_model.input, model(embedding_model.output))\n",
    "# nonlinear_beard_model.layers[-1].trainable = False # fix non-linear model and train only embeddings\n",
    "# nonlinear_beard_model.compile('sgd', 'mse')\n",
    "\n",
    "# nonlinear_beard_model.layers[1].set_weights([X_data[:10].reshape((-1, 18*512))])\n",
    "# y_data_real = nonlinear_beard_model.predict(np.arange(10))\n",
    "# # y_data_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BP06Td-MrGhb"
   },
   "outputs": [],
   "source": [
    "# factors = [-40, -20, 0, 20, 40]\n",
    "\n",
    "# for i, factor in enumerate(factors):\n",
    "#   fig,ax = plt.subplots(1, 10, figsize=(15, 10), dpi=160)\n",
    "#   nonlinear_beard_model.layers[1].set_weights([X_data[:10].reshape((-1, 18*512))])\n",
    "#   nonlinear_beard_model.fit(np.arange(10), np.full((10, 1), factor), verbose=0, epochs=500)\n",
    "#   for j, emb in enumerate(embedding_model.layers[0].get_weights()[0]):\n",
    "#     ax[j].imshow(generate_image(emb))\n",
    "# #     ax[j].set_title('Factor: %0.1f' % factor)\n",
    "#   [x.axis('off') for x in ax]\n",
    "#   plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "90iIwzraEzvv",
    "MxepNoS-E_Hp",
    "lvv3TeGYRO_w",
    "bi_9tPGGr3Q3",
    "HMa5Mn-7GtFx",
    "Jvb5wiycv5It"
   ],
   "machine_shape": "hm",
   "name": "StyleGAN.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
